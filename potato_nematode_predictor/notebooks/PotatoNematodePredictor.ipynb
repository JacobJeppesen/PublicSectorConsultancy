{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potato Nematode Predictor\n",
    "This work contains the public sector consultancy work on a potato nematode predictor carried out by Aarhus University.\n",
    "\n",
    "Start by configuring the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 23.7 ms\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import geopandas\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Automatically prints execution time for the individual cells\n",
    "%load_ext autotime\n",
    "\n",
    "# Automatically reloads functions defined in external files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# The path to the project (so absoute file paths can be used throughout the notebook)\n",
    "PROJ_PATH = Path.cwd().parent\n",
    "\n",
    "# Define which field polygons should be used for analysis (2017 to 2019 seem to follow the same metadata format)\n",
    "FIELD_POLYGONS = ['FieldPolygons2017', 'FieldPolygons2018', 'FieldPolygons2019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Download the field polygons from The Danish Agricultural Agency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2016.zip\n",
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2017.zip\n",
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2018.zip\n",
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2019.zip\n",
      "time: 24.3 ms\n"
     ]
    }
   ],
   "source": [
    "# Downloaded files will go into the 'data/external' folder\n",
    "dest_folder = PROJ_PATH / 'data' / 'external'\n",
    "if not dest_folder.exists():\n",
    "    os.makedirs(dest_folder)\n",
    "    \n",
    "# Define the download links for the field polygons for the individual years\n",
    "file_url_mapping = {\n",
    "    'FieldPolygons2016.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=3037da0f2744a85adc8b08ca5c31c3cb',\n",
    "    'FieldPolygons2017.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=d0c8946763e465bf9f6160a6bc40531f',\n",
    "    'FieldPolygons2018.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=cfb1b47130b7276f8515fbaae60bde2a',\n",
    "    'FieldPolygons2019.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=3d19613ac986ed05a7c301319738e332'\n",
    "}\n",
    "\n",
    "# Download the zipfiles\n",
    "for filename, url in file_url_mapping.items():\n",
    "    dest_path = PROJ_PATH / 'data' / 'external' / filename\n",
    "    if not dest_path.exists():\n",
    "        wget.download(url, str(dest_path))\n",
    "        print(\"File has been downloaded: \" + filename)\n",
    "    else:\n",
    "        print(\"File already exists: \" + str(PROJ_PATH / 'data' / 'external' / filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Then extract the zipfiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2019.zip\n",
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2016.zip\n",
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2017.zip\n",
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2018.zip\n",
      "time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "# The extracted zipfiles will go into the 'data/raw' folder\n",
    "for zipfile in (PROJ_PATH / 'data' / 'external').glob('**/*.zip'):\n",
    "    dest_folder = PROJ_PATH / 'data' / 'raw' / zipfile.stem   \n",
    "    if not dest_folder.exists():\n",
    "        with ZipFile(str(zipfile), 'r') as zipObj:\n",
    "            zipObj.extractall(str(dest_folder))\n",
    "        print(\"Zipfile has been extracted: \" + str(zipfile))\n",
    "    else:\n",
    "        print(\"Zipfile has already been extracted: \" + str(zipfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now load the shapefiles into geopandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "def load_shp(shp_name):\n",
    "    # Load shapefile into dataframe and remove NaN rows\n",
    "    shp_file_path = list((PROJ_PATH / 'data' / 'raw' / shp_name).glob('**/*.shp'))[0]\n",
    "    df = geopandas.read_file(str(shp_file_path))\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Change all column names to be lower-case to make the naming consistent across years (https://stackoverflow.com/a/36362607/12045808)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataframes into a dict, with each year as a key\n",
    "df_all = {}\n",
    "for df_name in FIELD_POLYGONS:\n",
    "    df = load_shp(df_name)\n",
    "    df_all[df_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Find the potato fields and count the number of unique sorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Analyzing FieldPolygons2017 ###\n",
      "There are 618 fields (total area = 4773 ha) of type: Kartofler, andre\n",
      "There are 88 fields (total area = 586 ha) of type: Kartofler, lægge- (certificerede)\n",
      "There are 1089 fields (total area = 6626 ha) of type: Kartofler, lægge- (egen opformering)\n",
      "There are 2597 fields (total area = 9223 ha) of type: Kartofler, spise-\n",
      "There are 3885 fields (total area = 28040 ha) of type: Kartofler, stivelses-\n",
      "\n",
      "### Analyzing FieldPolygons2018 ###\n",
      "There are 497 fields (total area = 3693 ha) of type: Kartofler, andre\n",
      "There are 317 fields (total area = 2988 ha) of type: Kartofler, lægge- (certificerede)\n",
      "There are 704 fields (total area = 3700 ha) of type: Kartofler, lægge- (egen opformering)\n",
      "There are 2137 fields (total area = 7805 ha) of type: Kartofler, spise-\n",
      "There are 3484 fields (total area = 26235 ha) of type: Kartofler, stivelses-\n",
      "\n",
      "### Analyzing FieldPolygons2019 ###\n",
      "There are 588 fields (total area = 4877 ha) of type: Kartofler, andre\n",
      "There are 465 fields (total area = 3966 ha) of type: Kartofler, lægge- (certificerede)\n",
      "There are 758 fields (total area = 4144 ha) of type: Kartofler, lægge- (egen opformering)\n",
      "There are 2185 fields (total area = 7930 ha) of type: Kartofler, spise-\n",
      "There are 4649 fields (total area = 35727 ha) of type: Kartofler, stivelses-\n",
      "\n",
      "time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "def extract_potato_fields(df):\n",
    "    # Create a new dataframe with all the different types of potatoes\n",
    "    df = df[df['afgroede'].str.contains(\"kartof\", case=False)]  \n",
    "\n",
    "    # Find the different potato types, count the number of fields for each type, and calculate total area for each type\n",
    "    for potato_type in sorted(df['afgroede'].unique()):\n",
    "        num_fields = df[df['afgroede'] == potato_type].shape[0]\n",
    "        sum_area = df[df['afgroede'] == potato_type]['imk_areal'].sum()\n",
    "        print(\"There are \" + str(num_fields) + \" fields (total area = \" + str(int(sum_area)) + \" ha) of type: \" + potato_type)\n",
    "        \n",
    "    return df \n",
    "\n",
    "# Extract the potato fields and load them into a new dict with each year as a key\n",
    "df_potato = {}\n",
    "for df_name, df in df_all.items():\n",
    "    print(\"### Analyzing \" + df_name + \" ###\")\n",
    "    df_potato[df_name] = extract_potato_fields(df)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Calculate zonal statistics for the the potato fields for the different radar data measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
