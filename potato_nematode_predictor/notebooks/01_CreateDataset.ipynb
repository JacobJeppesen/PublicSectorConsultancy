{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potato Nematode Predictor\n",
    "This work contains the public sector consultancy work on a potato nematode predictor carried out by Aarhus University.\n",
    "\n",
    "Start by configuring the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import geopandas\n",
    "import os\n",
    "import rasterio\n",
    "import sys\n",
    "import fiona\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from utils import RasterstatsMultiProc\n",
    "\n",
    "# Automatically prints execution time for the individual cells\n",
    "%load_ext autotime\n",
    "\n",
    "# Automatically reloads functions defined in external files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set xarray to use html as display_style\n",
    "xr.set_options(display_style=\"html\")\n",
    "\n",
    "# The path to the project (so absoute file paths can be used throughout the notebook)\n",
    "PROJ_PATH = Path.cwd().parent\n",
    "\n",
    "# Define which field polygons should be used for analysis (2017 to 2019 seem to follow the same metadata format)\n",
    "FIELD_POLYGONS = ['FieldPolygons2017', 'FieldPolygons2018', 'FieldPolygons2019']\n",
    "\n",
    "# Define global flags\n",
    "CROP_TYPES = ['Vårbyg',  'Vinterbyg', 'Vårhvede', 'Vinterhvede', 'Vinterrug', 'Vårhavre', 'Silomajs', 'Vinterraps', \n",
    "              'Permanent græs, normalt udbytte', 'Pil', 'Skovdrift, alm.']  \n",
    "\n",
    "ONLY_POTATO = False\n",
    "MULTI_PROC_ZONAL_STATS = False\n",
    "ALL_TOUCHED = False\n",
    "BUFFER_SIZE = -20  # Unit is meterJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Download the field polygons from The Danish Agricultural Agency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2016.zip\n",
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2017.zip\n",
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2018.zip\n",
      "File already exists: /home/jovyan/work/data/external/FieldPolygons2019.zip\n",
      "time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "# Downloaded files will go into the 'data/external' folder\n",
    "dest_folder = PROJ_PATH / 'data' / 'external'\n",
    "if not dest_folder.exists():\n",
    "    os.makedirs(dest_folder)\n",
    "    \n",
    "# Define the download links for the field polygons for the individual years\n",
    "file_url_mapping = {\n",
    "    'FieldPolygons2016.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=3037da0f2744a85adc8b08ca5c31c3cb',\n",
    "    'FieldPolygons2017.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=d0c8946763e465bf9f6160a6bc40531f',\n",
    "    'FieldPolygons2018.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=cfb1b47130b7276f8515fbaae60bde2a',\n",
    "    'FieldPolygons2019.zip': 'https://kortdata.fvm.dk/download/DownloadStream?id=3d19613ac986ed05a7c301319738e332'\n",
    "}\n",
    "\n",
    "# Download the zipfiles\n",
    "for filename, url in file_url_mapping.items():\n",
    "    dest_path = PROJ_PATH / 'data' / 'external' / filename\n",
    "    if not dest_path.exists():\n",
    "        wget.download(url, str(dest_path))\n",
    "        print(\"File has been downloaded: \" + filename)\n",
    "    else:\n",
    "        print(\"File already exists: \" + str(PROJ_PATH / 'data' / 'external' / filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Then extract the zipfiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2019.zip\n",
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2016.zip\n",
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2017.zip\n",
      "Zipfile has already been extracted: /home/jovyan/work/data/external/FieldPolygons2018.zip\n",
      "time: 19 ms\n"
     ]
    }
   ],
   "source": [
    "# The extracted zipfiles will go into the 'data/raw' folder\n",
    "for zipfile in (PROJ_PATH / 'data' / 'external').glob('**/*.zip'):\n",
    "    dest_folder = PROJ_PATH / 'data' / 'raw' / zipfile.stem   \n",
    "    if not dest_folder.exists():\n",
    "        with ZipFile(str(zipfile), 'r') as zipObj:\n",
    "            zipObj.extractall(str(dest_folder))\n",
    "        print(\"Zipfile has been extracted: \" + str(zipfile))\n",
    "    else:\n",
    "        print(\"Zipfile has already been extracted: \" + str(zipfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now the most common crop types for the individual years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Analyzing FieldPolygons2017 ###\n",
      "Crop type: Vårbyg (fields=94171, afgkode=1)\n",
      "Crop type: Vinterhvede (fields=69211, afgkode=11)\n",
      "Crop type: Permanent græs, normalt udbytte (fields=55260, afgkode=252)\n",
      "Crop type: Græs med kløver/lucerne, under 50 % bælgpl. (omdrift) (fields=46009, afgkode=260)\n",
      "Crop type: Silomajs (fields=26384, afgkode=216)\n",
      "Crop type: MFO-Slåningsbrak (fields=21435, afgkode=308)\n",
      "Crop type: Permanent græs og kløvergræs uden norm, under 50 % kløver (fields=20066, afgkode=276)\n",
      "Crop type: Vinterraps (fields=19646, afgkode=22)\n",
      "Crop type: Græs uden kløvergræs (omdrift) (fields=19455, afgkode=263)\n",
      "Crop type: Vinterbyg (fields=16976, afgkode=10)\n",
      "Crop type: Vinterhybridrug (fields=16080, afgkode=15)\n",
      "Crop type: Miljøgræs MVJ-tilsagn (0 N), permanent (fields=13252, afgkode=254)\n",
      "Crop type: Permanent græs, lavt udbytte (fields=12391, afgkode=251)\n",
      "Crop type: Vårhavre (fields=12261, afgkode=3)\n",
      "Crop type: Juletræer og pyntegrønt på landbrugsjord (fields=10290, afgkode=583)\n",
      "Crop type: Permanent græs, meget lavt udbytte (fields=8612, afgkode=250)\n",
      "Crop type: Miljøgræs MVJ-tilsagn (0 N), omdrift (fields=8467, afgkode=247)\n",
      "Crop type: MVJ ej udtagning, ej landbrugsareal (fields=8167, afgkode=318)\n",
      "Crop type: Græs under 50% kløver/lucerne, lavt udbytte (omdrift) (fields=7701, afgkode=268)\n",
      "Crop type: Permanent græs, uden kløver (fields=5816, afgkode=257)\n",
      "Crop type: Græs og kløvergræs uden norm, under 50 % kløver (omdrift) (fields=4858, afgkode=264)\n",
      "Crop type: Vinterrug (fields=4504, afgkode=14)\n",
      "Crop type: Rekreative formål (fields=4349, afgkode=271)\n",
      "Crop type: Permanent græs, under 50% kløver/lucerne (fields=4044, afgkode=255)\n",
      "Crop type: Kartofler, stivelses- (fields=3906, afgkode=151)\n",
      "Crop type: Grønkorn af vårbyg (fields=3558, afgkode=701)\n",
      "Crop type: Vårbyg, helsæd (fields=2868, afgkode=210)\n",
      "Crop type: Sukkerroer til fabrik (fields=2849, afgkode=160)\n",
      "Crop type: Vårhvede (fields=2656, afgkode=2)\n",
      "Crop type: Kartofler, spise- (fields=2644, afgkode=152)\n",
      "Crop type: Naturarealer, økologisk jordbrug (fields=2458, afgkode=907)\n",
      "Crop type: Rajgræsfrø, alm. (fields=2428, afgkode=101)\n",
      "Crop type: Slåningsbrak (fields=2314, afgkode=310)\n",
      "Crop type: Majs til modenhed (fields=2085, afgkode=5)\n",
      "Crop type: Vinterhvede, brødhvede (fields=2057, afgkode=13)\n",
      "Crop type: Skovdrift, alm. (fields=1985, afgkode=580)\n",
      "Crop type: Hestebønner (fields=1972, afgkode=31)\n",
      "Crop type: Skovrejsning på tidl. landbrugsjord 3 (fields=1909, afgkode=587)\n",
      "Crop type: Græs  under 50% kløver/lucerne, meget lavt udbytte (omdrift) (fields=1780, afgkode=267)\n",
      "Crop type: Rødsvingelfrø (fields=1695, afgkode=108)\n",
      "Crop type: 20-årig udtagning (fields=1648, afgkode=312)\n",
      "Crop type: Skovrejsning på tidl. landbrugsjord 1 (fields=1580, afgkode=311)\n",
      "Crop type: Korn og bælgsæd, helsæd, under 50% bælgsæd (fields=1379, afgkode=214)\n",
      "Crop type: Vintertriticale (fields=1366, afgkode=16)\n",
      "Crop type: Pil (fields=1330, afgkode=592)\n",
      "Crop type: Poppel (fields=1246, afgkode=593)\n",
      "Crop type: Vildtafgrøder (fields=1126, afgkode=360)\n",
      "Crop type: Æbler (fields=1119, afgkode=528)\n",
      "Crop type: MFO - Pil (fields=1113, afgkode=602)\n",
      "Crop type: Kartofler, lægge- (egen opformering) (fields=1096, afgkode=150)\n",
      "Crop type: Ærtehelsæd (fields=1036, afgkode=215)\n",
      "Crop type: Græs under 50% kløver/lucerne, ekstremt lavt udbytte (omdrift) (fields=1011, afgkode=266)\n",
      "Crop type: MFO-Blomsterbrak (fields=938, afgkode=325)\n",
      "Crop type: Fodersukkerroer (fields=897, afgkode=280)\n",
      "Crop type: Spinatfrø (fields=857, afgkode=124)\n",
      "Crop type: Jordbær (fields=840, afgkode=513)\n",
      "Crop type: Planteskolekulturer, vedplanter, til videresalg (fields=834, afgkode=497)\n",
      "Crop type: Miljøtiltag, ej landbrugsarealer (fields=793, afgkode=321)\n",
      "Crop type: Ærter (fields=787, afgkode=30)\n",
      "Crop type: MFO - Poppel (fields=720, afgkode=603)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True:  # Set it to True if you want to find the results\n",
    "    for df_name in FIELD_POLYGONS:\n",
    "        shp_path = list((PROJ_PATH / 'data' / 'raw' / df_name).glob('**/*.shp'))[0]\n",
    "        df = geopandas.read_file(str(shp_path))   \n",
    "        \n",
    "        # Change all column names to be lower-case to make the naming consistent across years (https://stackoverflow.com/a/36362607/12045808)\n",
    "        df.columns = map(str.lower, df.columns)\n",
    "\n",
    "        # Find most common crop types\n",
    "        n = 60 \n",
    "        crop_types = df['afgroede'].value_counts()[:n].index.tolist()\n",
    "        print(\"### Analyzing \" + df_name + \" ###\")\n",
    "        for crop_type in crop_types:\n",
    "            num_fields = df[df['afgroede'] == crop_type].shape[0]\n",
    "            afgkode = df[df['afgroede'] == crop_type].iloc[0]['afgkode']\n",
    "            print(\"Crop type: {} (fields={}, afgkode={})\".format(crop_type, num_fields, int(afgkode)))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Find the potato fields and count the number of unique sorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_and_analyze_fields(shp_path, only_potato=True, crop_types=['Vinterhvede']):\n",
    "    # Load shapefile into dataframe and remove NaN rows\n",
    "    df = geopandas.read_file(str(shp_path))\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Change all column names to be lower-case to make the naming consistent across years (https://stackoverflow.com/a/36362607/12045808)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    # Buffer the geometries to take imprecise coregistration into consideration (important for zonal statistics)\n",
    "    df['geometry'] = df['geometry'].values.buffer(BUFFER_SIZE)\n",
    "    df = df[~df['geometry'].is_empty]  # Filter away all empty polygons (ie. fields with zero area after buffering)\n",
    "    \n",
    "    # Extract crop types \n",
    "    max_fields_per_type = 5000\n",
    "    df_extracted = geopandas.GeoDataFrame(columns = df.columns, crs=df.crs)\n",
    "    potato_types = df[df['afgroede'].str.contains('kartof', case=False)]['afgroede'].unique()\n",
    "    for potato_type in potato_types:  \n",
    "        df_crop = df[df['afgroede'] == potato_type]\n",
    "        if df_crop.shape[0] > max_fields_per_type:  # Get a maximum of n fields for each crop type\n",
    "            df_crop = df_crop.sample(n=max_fields_per_type)  \n",
    "        df_extracted = df_extracted.append(df_crop)\n",
    "    \n",
    "    if not only_potato:\n",
    "        for crop_type in crop_types:  \n",
    "            df_crop = df[df['afgroede'] == crop_type]\n",
    "            if df_crop.shape[0] > max_fields_per_type:  # Get a maximum of n fields for each crop type\n",
    "                df_crop = df_crop.sample(n=max_fields_per_type)  \n",
    "            df_extracted = df_extracted.append(df_crop)\n",
    "    df = df_extracted\n",
    "    \n",
    "    # Find the total number of fields\n",
    "    num_fields = df.shape[0]\n",
    "    sum_area = df['imk_areal'].sum()\n",
    "    print(\"### Analyzing \" + df_name + \" (after buffering of \" + str(BUFFER_SIZE) + \"m) ###\")\n",
    "    print(\"There are a total of \" + str(num_fields) + \" fields (total area = \" + str(int(sum_area)) + \" ha)\")\n",
    "\n",
    "    # Find the different crop types, count the number of fields for each type, and calculate total area for each type\n",
    "    extracted_crop_types = df['afgroede'].unique()\n",
    "    for crop_type in sorted(extracted_crop_types):\n",
    "        num_fields = df[df['afgroede'] == crop_type].shape[0]\n",
    "        sum_area = df[df['afgroede'] == crop_type]['imk_areal'].sum()\n",
    "        print(\"There are \" + str(num_fields) + \" fields (total area = \" + str(int(sum_area)) + \" ha) of type: \" + crop_type)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer and analyze the field polygons\n",
    "for df_name in FIELD_POLYGONS:\n",
    "    shp_src_path = list((PROJ_PATH / 'data' / 'raw' / df_name).glob('**/*.shp'))[0]\n",
    "    shp_dest_name = '{}_buffered'.format(df_name)\n",
    "    shp_dest_path = (PROJ_PATH / 'data' / 'processed' / shp_dest_name / shp_dest_name).with_suffix('.shp')\n",
    "    \n",
    "    if not shp_dest_path.exists():\n",
    "        print(\"Buffering and analyzing field polygons: \" + df_name)\n",
    "        print(\"\")\n",
    "        df = buffer_and_analyze_fields(shp_src_path, only_potato=ONLY_POTATO, crop_types=CROP_TYPES)\n",
    "        \n",
    "        # Reproject the field polygons to the CRS of the tif files\n",
    "        tif = list((PROJ_PATH / 'data' / 'raw' / 'Sentinel-1').glob('*.tif'))[0]\n",
    "        with rasterio.open(tif) as src:\n",
    "            tif_crs = src.crs\n",
    "            #print(\"Projection used in tif: \" + str(tif_crs))\n",
    "        #df = df.to_crs({'init': tif_crs})\n",
    "        df = df.to_crs(tif_crs)\n",
    "        \n",
    "        # Set the CRS in the geodataframe to be wkt format (otherwise you won't be able to save as a shapefile)\n",
    "        #df.crs = df.crs['init'].to_wkt()\n",
    "        df.crs = df.crs.to_wkt()\n",
    "\n",
    "        if not shp_dest_path.parent.exists():\n",
    "            os.makedirs(shp_dest_path.parent)\n",
    "        df.to_file(shp_dest_path)\n",
    "    else:\n",
    "        print(\"Field polygons have already been buffered and analyzed: \" + df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Calculate zonal statistics for the the fields for the different radar data measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now want to create an xarray dataset based on the dataframe\n",
    "tifs = sorted((PROJ_PATH / 'data' / 'raw' / 'Sentinel-1').glob('*.tif'))\n",
    "\n",
    "for df_name in FIELD_POLYGONS: # Loop over all field polygon years\n",
    "    shp_name = '{}_buffered'.format(df_name)\n",
    "    shp_path = (PROJ_PATH / 'data' / 'processed' / shp_name / shp_name).with_suffix('.shp')\n",
    "    \n",
    "    netcdf_name = df_name + '_stats' \n",
    "    netcdf_path = (PROJ_PATH / 'data' / 'processed' / netcdf_name).with_suffix('.nc')\n",
    "    if netcdf_path.exists():\n",
    "    #if not '2019' in df_name:\n",
    "        print(\"Zonal statistics have already been calculated for: \" + df_name)\n",
    "    else:\n",
    "        print(\"Calculating zonal statistics for: \" + df_name)\n",
    "        ### HACKY WAY TO DO THIS - IT SHOULD BE DONE INSIDE RASTERSTATSMULTIPROC ###\n",
    "        # TODO: Figure out how to do this on the pandas df instead of opening features from the shape file\n",
    "        #       (ie. implement calc_zonal_stats_multiproc with the use of df - but df cannot be self.df - it must be parsed into the function)\n",
    "        with fiona.open(shp_path) as src:\n",
    "            features = list(src)\n",
    "            crs = src.crs\n",
    "        ###\n",
    "        \n",
    "        df = geopandas.read_file(str(shp_path))\n",
    "        ### FOR DEBUGGING ###\n",
    "        #df = df.head(200)  \n",
    "        #features = features[:200]\n",
    "        #tifs = tifs[0:3]\n",
    "        #####################\n",
    "        \n",
    "        # Load the dataframe into xarray \n",
    "        ds = xr.Dataset.from_dataframe(df.set_index('id'))  # Use field_id (named 'id') as index\n",
    "        ds = ds.rename({'id': 'field_id'})  \n",
    "        ds = ds.drop('geometry')  # Cannot be saved to netcdf format\n",
    "\n",
    "        # Find the dates of all the tif files and assign them as new coordinates\n",
    "        dates_str = list(map(lambda x: x.stem[4:12], tifs))\n",
    "        dates = pd.to_datetime(dates_str)\n",
    "        ds = ds.assign_coords({'date': dates})\n",
    "        \n",
    "        # Assign polarization coordinates\n",
    "        ds = ds.assign_coords({'polarization': ['VH', 'VV', 'VV-VH']})\n",
    "\n",
    "        # Create the empty arrays for the xarray data_vars\n",
    "        num_fields = ds.dims['field_id']\n",
    "        num_dates = len(dates)\n",
    "        num_polarizations = ds.dims['polarization']\n",
    "        stats_min_array = np.zeros((num_fields, num_dates, num_polarizations), dtype=np.float32)  \n",
    "        stats_max_array = np.zeros_like(stats_min_array)\n",
    "        stats_mean_array = np.zeros_like(stats_min_array)\n",
    "        stats_std_array = np.zeros_like(stats_min_array)\n",
    "        stats_median_array = np.zeros_like(stats_min_array)\n",
    "        satellite_array = [None] * num_dates\n",
    "        pass_mode_array = [None] * num_dates\n",
    "        relative_orbit_array = np.zeros((num_dates), dtype=np.int16)\n",
    "        \n",
    "        # Calculate the zonal stats\n",
    "        for date_index, tif in enumerate(tqdm(tifs)):  # Loop over all Sentinel-1 images\n",
    "            # Get metadata for satellite pass from the filename of the .tif file\n",
    "            satellite = tif.stem[0:3]\n",
    "            pass_mode = tif.stem[20:23]\n",
    "            relative_orbit = tif.stem[24:27]\n",
    "            \n",
    "            # Perform zonal statistics \n",
    "            for band in range(1, 4):  # Loop over all polarizations, including cross-polarization (indexed 1 to 3)\n",
    "                rasterstatsmulti = RasterstatsMultiProc(df=df, shp=shp_path, tif=tif, band=band, all_touched=ALL_TOUCHED)\n",
    "\n",
    "                if False:\n",
    "                #if MULTI_PROC_ZONAL_STATS:\n",
    "                    # Todo: Parse df to the function and use that instead of features\n",
    "                    # NOTE: MULTIPROC DOES NOT WORK! IT ONLY CALCULATES VH (IE. BAND 0) EVERY \n",
    "                    #       TIME, AND NEVER GET TO VV AND VV-VH (ie. BAND 1 AND 2)\n",
    "                    results_df = rasterstatsmulti.calc_zonal_stats_multiproc(features, crs)     \n",
    "                else:\n",
    "                    results_df = rasterstatsmulti.calc_zonal_stats(prog_bar=False) \n",
    "                    \n",
    "                del rasterstatsmulti\n",
    "\n",
    "                # Check if the ordering of the field_ids are the same in the xarray dataset and the results_df\n",
    "                # (they must be - otherwise the calculated statistics will be assigned to the wrong elements in the statistics arrays)\n",
    "                for i in np.random.randint(low=0, high=num_fields, size=20):\n",
    "                    ds_field_id = ds.isel(field_id=i)['field_id'].values\n",
    "                    df_field_id = results_df.iloc[i]['id']\n",
    "                    assert ds_field_id == df_field_id \n",
    "                \n",
    "                # Update the arrays\n",
    "                polarization_index = band-1  # Get the indexing right\n",
    "                stats_min_array[:, date_index, polarization_index] = results_df['min']\n",
    "                stats_max_array[:, date_index, polarization_index] = results_df['max']\n",
    "                stats_mean_array[:, date_index, polarization_index] = results_df['mean']\n",
    "                stats_std_array[:, date_index, polarization_index] = results_df['std']\n",
    "                stats_median_array[:, date_index, polarization_index] = results_df['median']\n",
    "                satellite_array[date_index] = satellite\n",
    "                pass_mode_array[date_index] = pass_mode \n",
    "                relative_orbit_array[date_index] = relative_orbit \n",
    "                \n",
    "        # Load the arrays into xarray\n",
    "        ds['stats_min']=(['field_id', 'date', 'polarization'], stats_min_array)\n",
    "        ds['stats_max']=(['field_id', 'date', 'polarization'], stats_max_array)\n",
    "        ds['stats_mean']=(['field_id', 'date', 'polarization'], stats_mean_array)\n",
    "        ds['stats_std']=(['field_id', 'date', 'polarization'], stats_std_array)\n",
    "        ds['stats_median']=(['field_id', 'date', 'polarization'], stats_median_array)\n",
    "        ds['satellite']=(['date'], satellite_array)\n",
    "        ds['pass_mode']=(['date'], pass_mode_array)\n",
    "        ds['relative_orbit']=(['date'], relative_orbit_array)\n",
    "        \n",
    "        # Use proper dtypes in the datset to save space and memory\n",
    "        ds['field_id'] = ds['field_id'].astype(np.int32) \n",
    "        ds['afgkode'] = ds['afgkode'].astype(np.int16) \n",
    "        ds['gb'] = ds['gb'].astype(np.float32) \n",
    "        ds['gbanmeldt'] = ds['gbanmeldt'].astype(np.float32) \n",
    "        ds['imk_areal'] = ds['imk_areal'].astype(np.float32) \n",
    "\n",
    "        # Save the dataset\n",
    "        if not netcdf_path.parent.exists():\n",
    "            os.makedirs(netcdf_path.parent)\n",
    "        ds = ds.sortby('date')  # Sort the dates (they are scrambled due to naming of the tif files starting with 'S1A' and 'S1B')\n",
    "        ds.to_netcdf(netcdf_path, engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and look at the saved dataset\n",
    "netcdf_path = (PROJ_PATH / 'data' / 'processed' / 'FieldPolygons2019_stats').with_suffix('.nc')\n",
    "ds = xr.open_dataset(netcdf_path, engine=\"h5netcdf\")\n",
    "ds  # Remember to close the dataset before the netcdf file can be rewritten in cells above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
